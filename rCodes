library(ggplot2)
library(ggthemes)
library(dplyr)
library(corrgram)
library(corrplot)
library(caTools)

Melb_Property <- read.csv('melbourne_housingv3.csv')
Suburbs <- read.csv('Suburbs in Melbourne.csv')

Melb_Sub_Price <- Melb_Property %>% group_by(Suburb,Distance,Sample) %>% summarise(Price = mean(Price, na.rm=T))

# Set a Seed
#set.seed(101)

# split up sample. The first data called is the data that you want to predict (i.e. df$G3)
# I.e sample <- sample.split(Melb_Sub_Price$Price,SplitRatio = 0.7)
# So what happens is that a new column (called sample) will be created where the bolean will be True ot False.
train <- subset(Melb_Sub_Price,Sample == TRUE)
# So you then get all those (70%) data that was identified for training (i.e. sample == TRUE)
test <- subset(Melb_Sub_Price,Sample==FALSE)
# same otherwise for the test data.

# Building and Training Model
# the formula of the model is lm(y ~ x1 + x2,data)
# if you want to use ALL the features in the df then, lm(y ~. , data)
model_house <- lm(Price ~., data = train)

# Predicting the Model
Price.predictions <- predict(model_house,test)

results <- cbind(Price.predictions,test$Price)
colnames(results) <- c('predicted','actual')
results <- as.data.frame(results)
#print(head(results))

# Take care of NEG VALUES
to_zero <- function(x){
  if (x<0) {
    return(0)
  } else {
    return(x)
  }
}  

# APPLY Zero Function
results$predicted <- sapply(results$predicted,to_zero)
print('MSE')
# Mean Squared Error (to measure how off the model is)
mse <- mean((results$actual - results$predicted)^2)
print(mse)

# RMSE
print('Squared Root of MSE')
print(mse^0.5)


SSE <- sum((results$predicted - results$actual)^2)
SST <- sum((mean(Melb_Sub_Price$Price) - results$actual)^2)

R2 <- 1 - SSE/SST
print('R2')
print(R2)



